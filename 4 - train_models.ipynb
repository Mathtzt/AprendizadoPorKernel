{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5db1e8ed",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f61f352e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-24 18:14:18.777818: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "import Models\n",
    "from AuxiliarFunctions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3673cc4f",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e184bb0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hash_id</th>\n",
       "      <th>label</th>\n",
       "      <th>raw_files_path</th>\n",
       "      <th>processed_file_folder</th>\n",
       "      <th>processed_file_path</th>\n",
       "      <th>cv_alg</th>\n",
       "      <th>cv_folds</th>\n",
       "      <th>cv_path</th>\n",
       "      <th>preproc_alg</th>\n",
       "      <th>pipeline_path</th>\n",
       "      <th>scaler_alg</th>\n",
       "      <th>train_data_path</th>\n",
       "      <th>train_trgt_path</th>\n",
       "      <th>model_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6111500007297378247</td>\n",
       "      <td>Toy Data Classification with 10 StratifiedKFolds</td>\n",
       "      <td>data/raw</td>\n",
       "      <td>data</td>\n",
       "      <td>data/6111500007297378247_processed_data.csv</td>\n",
       "      <td>StratifiedKFolds</td>\n",
       "      <td>10</td>\n",
       "      <td>data/indexes</td>\n",
       "      <td>Não implementado para a aplicação!!!</td>\n",
       "      <td>data/pipelines</td>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>data/6111500007297378247_train_data.csv</td>\n",
       "      <td>data/6111500007297378247_trgt_data.csv</td>\n",
       "      <td>data/models</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               hash_id                                             label  \\\n",
       "0  6111500007297378247  Toy Data Classification with 10 StratifiedKFolds   \n",
       "\n",
       "  raw_files_path processed_file_folder  \\\n",
       "0       data/raw                  data   \n",
       "\n",
       "                           processed_file_path            cv_alg  cv_folds  \\\n",
       "0  data/6111500007297378247_processed_data.csv  StratifiedKFolds        10   \n",
       "\n",
       "        cv_path                           preproc_alg   pipeline_path  \\\n",
       "0  data/indexes  Não implementado para a aplicação!!!  data/pipelines   \n",
       "\n",
       "       scaler_alg                          train_data_path  \\\n",
       "0  StandardScaler  data/6111500007297378247_train_data.csv   \n",
       "\n",
       "                          train_trgt_path   model_path  \n",
       "0  data/6111500007297378247_trgt_data.csv  data/models  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_file_path = 'data/config.csv'\n",
    "df_config = pd.read_csv(config_file_path)\n",
    "train_id = 0\n",
    "\n",
    "df_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95a5ecaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.read_csv(df_config['train_data_path'][train_id])\n",
    "df_trgt = pd.read_csv(df_config['train_trgt_path'][train_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cf290a",
   "metadata": {},
   "source": [
    "# Processo de Treinamento de um modelo simples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "25a750ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing SVM Training\n",
      "Processing Training for linear kernel\n",
      "Processing Training for 0.5 regularization\n",
      "Training 1 fold of 10 folds\n",
      "\n",
      "\n",
      "Reading Cross-validation indexes\n",
      "Done\n",
      "Reading Pipeline Object\n",
      "Done\n",
      "Training for model\n",
      "No Model \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6111500007297378247_SVM_0_fold_model_linear_kernel_0-5_regularization_prediction_file.csv\n",
      "Training 2 fold of 10 folds\n",
      "\n",
      "\n",
      "Reading Cross-validation indexes\n",
      "Done\n",
      "Reading Pipeline Object\n",
      "Done\n",
      "Training for model\n",
      "No Model \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6111500007297378247_SVM_1_fold_model_linear_kernel_0-5_regularization_prediction_file.csv\n",
      "Training 3 fold of 10 folds\n",
      "\n",
      "\n",
      "Reading Cross-validation indexes\n",
      "Done\n",
      "Reading Pipeline Object\n",
      "Done\n",
      "Training for model\n",
      "No Model \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6111500007297378247_SVM_2_fold_model_linear_kernel_0-5_regularization_prediction_file.csv\n",
      "Training 4 fold of 10 folds\n",
      "\n",
      "\n",
      "Reading Cross-validation indexes\n",
      "Done\n",
      "Reading Pipeline Object\n",
      "Done\n",
      "Training for model\n",
      "No Model \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6111500007297378247_SVM_3_fold_model_linear_kernel_0-5_regularization_prediction_file.csv\n",
      "Training 5 fold of 10 folds\n",
      "\n",
      "\n",
      "Reading Cross-validation indexes\n",
      "Done\n",
      "Reading Pipeline Object\n",
      "Done\n",
      "Training for model\n",
      "No Model \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6111500007297378247_SVM_4_fold_model_linear_kernel_0-5_regularization_prediction_file.csv\n",
      "Training 6 fold of 10 folds\n",
      "\n",
      "\n",
      "Reading Cross-validation indexes\n",
      "Done\n",
      "Reading Pipeline Object\n",
      "Done\n",
      "Training for model\n",
      "No Model \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6111500007297378247_SVM_5_fold_model_linear_kernel_0-5_regularization_prediction_file.csv\n",
      "Training 7 fold of 10 folds\n",
      "\n",
      "\n",
      "Reading Cross-validation indexes\n",
      "Done\n",
      "Reading Pipeline Object\n",
      "Done\n",
      "Training for model\n",
      "No Model \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6111500007297378247_SVM_6_fold_model_linear_kernel_0-5_regularization_prediction_file.csv\n",
      "Training 8 fold of 10 folds\n",
      "\n",
      "\n",
      "Reading Cross-validation indexes\n",
      "Done\n",
      "Reading Pipeline Object\n",
      "Done\n",
      "Training for model\n",
      "No Model \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6111500007297378247_SVM_7_fold_model_linear_kernel_0-5_regularization_prediction_file.csv\n",
      "Training 9 fold of 10 folds\n",
      "\n",
      "\n",
      "Reading Cross-validation indexes\n",
      "Done\n",
      "Reading Pipeline Object\n",
      "Done\n",
      "Training for model\n",
      "No Model \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6111500007297378247_SVM_8_fold_model_linear_kernel_0-5_regularization_prediction_file.csv\n",
      "Training 10 fold of 10 folds\n",
      "\n",
      "\n",
      "Reading Cross-validation indexes\n",
      "Done\n",
      "Reading Pipeline Object\n",
      "Done\n",
      "Training for model\n",
      "No Model \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6111500007297378247_SVM_9_fold_model_linear_kernel_0-5_regularization_prediction_file.csv\n",
      "Processing Training for 0.2 regularization\n",
      "Training 1 fold of 10 folds\n",
      "\n",
      "\n",
      "Reading Cross-validation indexes\n",
      "Done\n",
      "Reading Pipeline Object\n",
      "Done\n",
      "Training for model\n",
      "No Model \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6111500007297378247_SVM_0_fold_model_linear_kernel_0-2_regularization_prediction_file.csv\n",
      "Training 2 fold of 10 folds\n",
      "\n",
      "\n",
      "Reading Cross-validation indexes\n",
      "Done\n",
      "Reading Pipeline Object\n",
      "Done\n",
      "Training for model\n",
      "No Model \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6111500007297378247_SVM_1_fold_model_linear_kernel_0-2_regularization_prediction_file.csv\n",
      "Training 3 fold of 10 folds\n",
      "\n",
      "\n",
      "Reading Cross-validation indexes\n",
      "Done\n",
      "Reading Pipeline Object\n",
      "Done\n",
      "Training for model\n",
      "No Model \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6111500007297378247_SVM_2_fold_model_linear_kernel_0-2_regularization_prediction_file.csv\n",
      "Training 4 fold of 10 folds\n",
      "\n",
      "\n",
      "Reading Cross-validation indexes\n",
      "Done\n",
      "Reading Pipeline Object\n",
      "Done\n",
      "Training for model\n",
      "No Model \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6111500007297378247_SVM_3_fold_model_linear_kernel_0-2_regularization_prediction_file.csv\n",
      "Training 5 fold of 10 folds\n",
      "\n",
      "\n",
      "Reading Cross-validation indexes\n",
      "Done\n",
      "Reading Pipeline Object\n",
      "Done\n",
      "Training for model\n",
      "No Model \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6111500007297378247_SVM_4_fold_model_linear_kernel_0-2_regularization_prediction_file.csv\n",
      "Training 6 fold of 10 folds\n",
      "\n",
      "\n",
      "Reading Cross-validation indexes\n",
      "Done\n",
      "Reading Pipeline Object\n",
      "Done\n",
      "Training for model\n",
      "No Model \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6111500007297378247_SVM_5_fold_model_linear_kernel_0-2_regularization_prediction_file.csv\n",
      "Training 7 fold of 10 folds\n",
      "\n",
      "\n",
      "Reading Cross-validation indexes\n",
      "Done\n",
      "Reading Pipeline Object\n",
      "Done\n",
      "Training for model\n",
      "No Model \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6111500007297378247_SVM_6_fold_model_linear_kernel_0-2_regularization_prediction_file.csv\n",
      "Training 8 fold of 10 folds\n",
      "\n",
      "\n",
      "Reading Cross-validation indexes\n",
      "Done\n",
      "Reading Pipeline Object\n",
      "Done\n",
      "Training for model\n",
      "No Model \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6111500007297378247_SVM_7_fold_model_linear_kernel_0-2_regularization_prediction_file.csv\n",
      "Training 9 fold of 10 folds\n",
      "\n",
      "\n",
      "Reading Cross-validation indexes\n",
      "Done\n",
      "Reading Pipeline Object\n",
      "Done\n",
      "Training for model\n",
      "No Model \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6111500007297378247_SVM_8_fold_model_linear_kernel_0-2_regularization_prediction_file.csv\n",
      "Training 10 fold of 10 folds\n",
      "\n",
      "\n",
      "Reading Cross-validation indexes\n",
      "Done\n",
      "Reading Pipeline Object\n",
      "Done\n",
      "Training for model\n",
      "No Model \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6111500007297378247_SVM_9_fold_model_linear_kernel_0-2_regularization_prediction_file.csv\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import copy\n",
    "\n",
    "model_type = 'SVM'\n",
    "n_folds =  df_config['cv_folds'][train_id]\n",
    "\n",
    "cv_path = df_config['cv_path'][train_id]\n",
    "pipe_path = df_config['pipeline_path'][train_id]\n",
    "model_path = df_config['model_path'][train_id]\n",
    "\n",
    "print('Processing SVM Training')\n",
    "\n",
    "kernels = ['linear']\n",
    "regularizations = [0.5, 0.2]\n",
    "\n",
    "train_record = None\n",
    "\n",
    "\n",
    "for kernel in kernels:\n",
    "    print('Processing Training for %s kernel'%(kernel))\n",
    "    for regularization in regularizations:\n",
    "        print('Processing Training for %s regularization'%(regularization))\n",
    "        for ifold in range(n_folds):\n",
    "            #if ifold >= 2: # para desenvolvimento\n",
    "            #    break\n",
    "            print('Training %i fold of %i folds\\n\\n'%(ifold+1, n_folds))\n",
    "\n",
    "            print('Reading Cross-validation indexes')\n",
    "\n",
    "            cv_name = '%s_%s_CV_fold_%i_of_%i_cv_indexes.pkl'%(df_config['hash_id'][train_id],\n",
    "                                                               df_config['cv_alg'][train_id],\n",
    "                                                               ifold, n_folds)\n",
    "            print('Done')\n",
    "            print('Reading Pipeline Object')\n",
    "            with open(os.path.join(cv_path,cv_name),'rb') as file_handler:\n",
    "                [trn_idx,val_idx] = pickle.load(file_handler)\n",
    "\n",
    "            pipe_name ='%s_%s_CV_fold_%i_of_%i_pipe.pkl'%(df_config['hash_id'][train_id],\n",
    "                                                               df_config['cv_alg'][train_id],\n",
    "                                                               ifold, n_folds)\n",
    "            with open(os.path.join(pipe_path,pipe_name),'rb') as file_handler:\n",
    "                pipe = joblib.load(file_handler)\n",
    "\n",
    "            print('Done')\n",
    "\n",
    "            trn_data = pipe.transform(df_data)\n",
    "            trn_trgt = df_trgt.values # tf.keras.utils.to_categorical(dev_target, num_classes=len(np.unique(dev_target)))\n",
    "\n",
    "            print('Training for model')\n",
    "            model_name = '%s_%s_%i_fold_model_%s_kernel_%s_regularization.pkl'%(df_config['hash_id'][train_id],\n",
    "                                                                                model_type, ifold, kernel, \n",
    "                                                                                str(regularization).replace('.','-'))\n",
    "\n",
    "            if os.path.exists(os.path.join(model_path, model_name)):\n",
    "                print('Model is in %s'%(os.path.join(model_path, model_name)))\n",
    "            else:\n",
    "                print('No Model \\n\\n')\n",
    "\n",
    "                model = Models.SVMClassificationModel(kernel='linear', regularization=0.5, verbose=False)\n",
    "                model.fit(trn_data, trn_trgt, trn_id=trn_idx, val_id=val_idx, random_state=0,)\n",
    "                predictions = model.predict(trn_data)\n",
    "                df_predict = pd.DataFrame(data=np.concatenate((trn_trgt, \n",
    "                                                               predictions[:,np.newaxis]),\n",
    "                                                              axis=1), \n",
    "                                          columns=['target', 'model_output'])\n",
    "                prediction_name = copy.copy(model_name)\n",
    "                prediction_name = prediction_name.replace('.pkl','_prediction_file.csv')\n",
    "                print(prediction_name)\n",
    "                df_predict.to_csv(os.path.join(model_path, prediction_name),index=False)\n",
    "               \n",
    "                model.save(os.path.join(model_path, model_name))\n",
    "                \n",
    "                acc = Models.acc_score(df_predict.loc[trn_idx,'target'],\n",
    "                                       df_predict.loc[trn_idx,'model_output'])\n",
    "                sens = Models.sensitivity_score(df_predict.loc[trn_idx,'target'],\n",
    "                                                df_predict.loc[trn_idx,'model_output'])\n",
    "                spec = Models.specificity_score(df_predict.loc[trn_idx,'target'],\n",
    "                                                df_predict.loc[trn_idx,'model_output'])\n",
    "                sp = Models.sp_index(df_predict.loc[trn_idx,'target'],\n",
    "                                     df_predict.loc[trn_idx,'model_output'])\n",
    "                auc = Models.auc_score(df_predict.loc[trn_idx,'target'],\n",
    "                                       df_predict.loc[trn_idx,'model_output'])\n",
    "                \n",
    "                dict_train_record = {\n",
    "                    'hash_id':[df_config['hash_id'][train_id]],'fold':[ifold],\n",
    "                    'prediction_file':[prediction_name], 'kernel':[kernel],\n",
    "                    'regularization':[regularization], 'Acc':[acc],\n",
    "                    'Sens':[sens],'Spec':[spec],'SP':[sp], 'AUC':[auc]\n",
    "                }\n",
    "                if train_record is None:\n",
    "                    train_record = pd.DataFrame(data=dict_train_record)\n",
    "                else:\n",
    "                    train_record = pd.concat([train_record,pd.DataFrame(data=dict_train_record)],axis=0, ignore_index=True)\n",
    "train_record.to_csv(os.path.join(model_path, 'train_record'),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c35e7976",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_grouped = train_record[['kernel','regularization',\n",
    "                        'Acc','Sens','Spec','SP', 'AUC']].groupby(['kernel', 'regularization']).mean()\n",
    "std_grouped = train_record[['kernel','regularization',\n",
    "                        'Acc','Sens','Spec','SP', 'AUC']].groupby(['kernel', 'regularization']).std()\n",
    "grouped = pd.concat([mean_grouped,std_grouped],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8a4c732e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Acc</th>\n",
       "      <th>Sens</th>\n",
       "      <th>Spec</th>\n",
       "      <th>SP</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kernel</th>\n",
       "      <th>regularization</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">linear</th>\n",
       "      <th>0.2</th>\n",
       "      <td>0.746500</td>\n",
       "      <td>0.780865</td>\n",
       "      <td>0.712094</td>\n",
       "      <td>0.556639</td>\n",
       "      <td>0.746479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.5</th>\n",
       "      <td>0.746500</td>\n",
       "      <td>0.780865</td>\n",
       "      <td>0.712094</td>\n",
       "      <td>0.556639</td>\n",
       "      <td>0.746479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.2</th>\n",
       "      <td>0.001304</td>\n",
       "      <td>0.002833</td>\n",
       "      <td>0.002777</td>\n",
       "      <td>0.001950</td>\n",
       "      <td>0.001306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.5</th>\n",
       "      <td>0.001304</td>\n",
       "      <td>0.002833</td>\n",
       "      <td>0.002777</td>\n",
       "      <td>0.001950</td>\n",
       "      <td>0.001306</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Acc      Sens      Spec        SP       AUC\n",
       "kernel regularization                                                  \n",
       "linear 0.2             0.746500  0.780865  0.712094  0.556639  0.746479\n",
       "       0.5             0.746500  0.780865  0.712094  0.556639  0.746479\n",
       "       0.2             0.001304  0.002833  0.002777  0.001950  0.001306\n",
       "       0.5             0.001304  0.002833  0.002777  0.001950  0.001306"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ab038456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hash_id</th>\n",
       "      <th>fold</th>\n",
       "      <th>prediction_file</th>\n",
       "      <th>kernel</th>\n",
       "      <th>regularization</th>\n",
       "      <th>Acc</th>\n",
       "      <th>Sens</th>\n",
       "      <th>Spec</th>\n",
       "      <th>SP</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6111500007297378247</td>\n",
       "      <td>0</td>\n",
       "      <td>6111500007297378247_SVM_0_fold_model_linear_ke...</td>\n",
       "      <td>linear</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.746889</td>\n",
       "      <td>0.783652</td>\n",
       "      <td>0.710093</td>\n",
       "      <td>0.557142</td>\n",
       "      <td>0.746873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6111500007297378247</td>\n",
       "      <td>1</td>\n",
       "      <td>6111500007297378247_SVM_1_fold_model_linear_ke...</td>\n",
       "      <td>linear</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.747778</td>\n",
       "      <td>0.781208</td>\n",
       "      <td>0.714317</td>\n",
       "      <td>0.558590</td>\n",
       "      <td>0.747763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6111500007297378247</td>\n",
       "      <td>2</td>\n",
       "      <td>6111500007297378247_SVM_2_fold_model_linear_ke...</td>\n",
       "      <td>linear</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.748000</td>\n",
       "      <td>0.779431</td>\n",
       "      <td>0.716541</td>\n",
       "      <td>0.558988</td>\n",
       "      <td>0.747986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6111500007297378247</td>\n",
       "      <td>3</td>\n",
       "      <td>6111500007297378247_SVM_3_fold_model_linear_ke...</td>\n",
       "      <td>linear</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.747000</td>\n",
       "      <td>0.779702</td>\n",
       "      <td>0.714254</td>\n",
       "      <td>0.557441</td>\n",
       "      <td>0.746978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6111500007297378247</td>\n",
       "      <td>4</td>\n",
       "      <td>6111500007297378247_SVM_4_fold_model_linear_ke...</td>\n",
       "      <td>linear</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.745222</td>\n",
       "      <td>0.775039</td>\n",
       "      <td>0.715366</td>\n",
       "      <td>0.554881</td>\n",
       "      <td>0.745202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6111500007297378247</td>\n",
       "      <td>5</td>\n",
       "      <td>6111500007297378247_SVM_5_fold_model_linear_ke...</td>\n",
       "      <td>linear</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.745667</td>\n",
       "      <td>0.780813</td>\n",
       "      <td>0.710474</td>\n",
       "      <td>0.555365</td>\n",
       "      <td>0.745643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6111500007297378247</td>\n",
       "      <td>6</td>\n",
       "      <td>6111500007297378247_SVM_6_fold_model_linear_ke...</td>\n",
       "      <td>linear</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.746778</td>\n",
       "      <td>0.781923</td>\n",
       "      <td>0.711586</td>\n",
       "      <td>0.557023</td>\n",
       "      <td>0.746754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6111500007297378247</td>\n",
       "      <td>7</td>\n",
       "      <td>6111500007297378247_SVM_7_fold_model_linear_ke...</td>\n",
       "      <td>linear</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.745000</td>\n",
       "      <td>0.780591</td>\n",
       "      <td>0.709362</td>\n",
       "      <td>0.554355</td>\n",
       "      <td>0.744976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6111500007297378247</td>\n",
       "      <td>8</td>\n",
       "      <td>6111500007297378247_SVM_8_fold_model_linear_ke...</td>\n",
       "      <td>linear</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.748111</td>\n",
       "      <td>0.785920</td>\n",
       "      <td>0.710251</td>\n",
       "      <td>0.558916</td>\n",
       "      <td>0.748086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6111500007297378247</td>\n",
       "      <td>9</td>\n",
       "      <td>6111500007297378247_SVM_9_fold_model_linear_ke...</td>\n",
       "      <td>linear</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.744556</td>\n",
       "      <td>0.780369</td>\n",
       "      <td>0.708695</td>\n",
       "      <td>0.553685</td>\n",
       "      <td>0.744532</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               hash_id  fold  \\\n",
       "0  6111500007297378247     0   \n",
       "1  6111500007297378247     1   \n",
       "2  6111500007297378247     2   \n",
       "3  6111500007297378247     3   \n",
       "4  6111500007297378247     4   \n",
       "5  6111500007297378247     5   \n",
       "6  6111500007297378247     6   \n",
       "7  6111500007297378247     7   \n",
       "8  6111500007297378247     8   \n",
       "9  6111500007297378247     9   \n",
       "\n",
       "                                     prediction_file  kernel  regularization  \\\n",
       "0  6111500007297378247_SVM_0_fold_model_linear_ke...  linear             0.2   \n",
       "1  6111500007297378247_SVM_1_fold_model_linear_ke...  linear             0.2   \n",
       "2  6111500007297378247_SVM_2_fold_model_linear_ke...  linear             0.2   \n",
       "3  6111500007297378247_SVM_3_fold_model_linear_ke...  linear             0.2   \n",
       "4  6111500007297378247_SVM_4_fold_model_linear_ke...  linear             0.2   \n",
       "5  6111500007297378247_SVM_5_fold_model_linear_ke...  linear             0.2   \n",
       "6  6111500007297378247_SVM_6_fold_model_linear_ke...  linear             0.2   \n",
       "7  6111500007297378247_SVM_7_fold_model_linear_ke...  linear             0.2   \n",
       "8  6111500007297378247_SVM_8_fold_model_linear_ke...  linear             0.2   \n",
       "9  6111500007297378247_SVM_9_fold_model_linear_ke...  linear             0.2   \n",
       "\n",
       "        Acc      Sens      Spec        SP       AUC  \n",
       "0  0.746889  0.783652  0.710093  0.557142  0.746873  \n",
       "1  0.747778  0.781208  0.714317  0.558590  0.747763  \n",
       "2  0.748000  0.779431  0.716541  0.558988  0.747986  \n",
       "3  0.747000  0.779702  0.714254  0.557441  0.746978  \n",
       "4  0.745222  0.775039  0.715366  0.554881  0.745202  \n",
       "5  0.745667  0.780813  0.710474  0.555365  0.745643  \n",
       "6  0.746778  0.781923  0.711586  0.557023  0.746754  \n",
       "7  0.745000  0.780591  0.709362  0.554355  0.744976  \n",
       "8  0.748111  0.785920  0.710251  0.558916  0.748086  \n",
       "9  0.744556  0.780369  0.708695  0.553685  0.744532  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_record"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4601fee2",
   "metadata": {},
   "source": [
    "# Treinamento de um modelo simples para decisão do médico"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727f496b",
   "metadata": {},
   "source": [
    "Durante o processo de desenvolvimento, tive uma discussão com o André sobre o processo de tomada de decisão de um médico e ele se baseia em 3 variáveis: proteina, citometria e dif cif. Ou seja, será que um modelo consegue fazer o mesmo que um médico apenas analisando estas variáveis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95a8e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model_type = 'DecisionTree'\n",
    "n_folds =  df_config['cv_folds'][train_id]\n",
    "\n",
    "cv_path = df_config['cv_path'][train_id]\n",
    "pipe_path = df_config['pipeline_path'][train_id]\n",
    "\n",
    "n_folds = df_config['cv_folds'][train_id]\n",
    "\n",
    "dataset_name = 'pos_test'\n",
    "\n",
    "print(get_train_description(df_config, train_id))\n",
    "\n",
    "print('Processing Decision Tree Training')\n",
    "data = dev_pos_test\n",
    "for ifold in range(n_folds):\n",
    "    #if ifold != 0: # para desenvolvimento\n",
    "    #    break\n",
    "\n",
    "    print('Training %i fold of %i folds\\n\\n'%(ifold+1, n_folds))\n",
    "\n",
    "    cv_name = '%s_%s_CV_fold_%i_of_%i_cv_indexes.pkl'%(df_config['hash_id'][train_id],\n",
    "                                                     df_config['cv_alg'][train_id],\n",
    "                                                     ifold, n_folds)\n",
    "    with open(os.path.join(cv_path,cv_name),'rb') as file_handler:\n",
    "        [trn_idx,val_idx] = pickle.load(file_handler)\n",
    "\n",
    "    pipe_name = '%s_%s_CV_fold_%i_of_%i_pipe_%s.jbl'%(df_config['hash_id'][train_id],\n",
    "                                                      df_config['cv_alg'][train_id],\n",
    "                                                      ifold, n_folds, \n",
    "                                                      dataset_name)\n",
    "    with open(os.path.join(pipe_path,pipe_name),'rb') as file_handler:\n",
    "        pipe = joblib.load(file_handler)\n",
    "\n",
    "    trn_data = pipe.transform(data)\n",
    "    trn_trgt = dev_target.values # tf.keras.utils.to_categorical(dev_target, num_classes=len(np.unique(dev_target)))\n",
    "    print('Training for model')\n",
    "    model_name = '%s_%s_%i_fold_model_%s.pkl'%(df_config['hash_id'][train_id],\n",
    "                                               model_type, ifold, \n",
    "                                               dataset_name)\n",
    "    model_path = df_config['model_path'][train_id]\n",
    "    if os.path.exists(os.path.join(model_path, model_name)):\n",
    "        print('Modelo existente em %s'%(os.path.join(model_path, model_name)))\n",
    "    else:\n",
    "        print('Modelo não existe\\n\\n')\n",
    "        model = DecisionTreeClassifier(max_leaf_nodes=4, random_state=0)\n",
    "        model.fit(trn_data, trn_trgt)\n",
    "        predictions = model.predict(trn_data)\n",
    "        df_predict = pd.DataFrame(data=np.concatenate((dev_target.values, \n",
    "                                                       predictions[:,np.newaxis]),\n",
    "                                                      axis=1), \n",
    "                                  columns=['target', 'model_output'])\n",
    "        prediction_name = '%s_%s_%i_fold_prediction_file_%s.csv'%(df_config['hash_id'][train_id],\n",
    "                                                                  model_type, ifold,\n",
    "                                                                  dataset_name)\n",
    "        df_predict.to_csv(os.path.join(model_path, prediction_name),index=False)\n",
    "        \n",
    "        pickle.dump(model, open(os.path.join(model_path, model_name), 'wb'))\n",
    "        # to load: model = pickle.load(open((os.path.join(model_path, model_name), 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8f4871",
   "metadata": {},
   "source": [
    "## Análise Básica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0ff151",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'DecisionTree'\n",
    "\n",
    "dataset_name = 'pos_test'#'personal', 'social', 'clinical', 'pos_test', 'PeSo', 'PeSoCli', 'PeCli', 'CliPos', 'PeSoCliPos_par'\n",
    "figure_of_merit_names = ['sens', 'spec', 'acc', 'sp']\n",
    "\n",
    "n_folds =  df_config['cv_folds'][train_id]\n",
    "cv_path = df_config['cv_path'][train_id]\n",
    "pipe_path = df_config['pipeline_path'][train_id]\n",
    "model_path = df_config['model_path'][train_id]\n",
    "\n",
    "df_train = pd.DataFrame(data=np.zeros([df_config['cv_folds'][train_id],len(figure_of_merit_names)]), columns=figure_of_merit_names)\n",
    "df_val = pd.DataFrame(np.zeros([df_config['cv_folds'][train_id],len(figure_of_merit_names)]))\n",
    "\n",
    "for ifold in range(df_config['cv_folds'][train_id]):\n",
    "    print('Processing fold %i of %i'%(ifold, df_config['cv_folds'][train_id]))\n",
    "    \n",
    "    cv_name = '%s_%s_CV_fold_%i_of_%i_cv_indexes.pkl'%(df_config['hash_id'][train_id],\n",
    "                                                         df_config['cv_alg'][train_id],\n",
    "                                                         ifold, n_folds)\n",
    "    with open(os.path.join(cv_path,cv_name),'rb') as file_handler:\n",
    "        [trn_idx,val_idx] = pickle.load(file_handler)\n",
    "    \n",
    "    prediction_file_name = '%s_%s_%i_fold_prediction_file_%s.csv'%(df_config['hash_id'][train_id],\n",
    "                                                                  model_type, ifold,\n",
    "                                                                  dataset_name)\n",
    "    print('Reading: ',prediction_file_name)\n",
    "    df_predict = pd.read_csv(os.path.join(model_path, prediction_file_name))\n",
    "    #print(df_predict['nn_output'].head())\n",
    "    #print('\\n')\n",
    "    for figure_of_merit in figure_of_merit_names:\n",
    "        \n",
    "        if figure_of_merit == 'sens':\n",
    "            df_train.loc[ifold,figure_of_merit] = Models.sensitivity_score(df_predict.loc[trn_idx,'target'], \n",
    "                                                                           df_predict.loc[trn_idx,'model_output'])\n",
    "            df_val.loc[ifold,figure_of_merit] = Models.sensitivity_score(df_predict.loc[val_idx,'target'], \n",
    "                                                                         df_predict.loc[val_idx,'model_output'])\n",
    "        if figure_of_merit == 'spec':\n",
    "            df_train.loc[ifold,figure_of_merit] = Models.specificity_score(df_predict.loc[trn_idx,'target'], \n",
    "                                                                           df_predict.loc[trn_idx,'model_output'])\n",
    "            df_val.loc[ifold,figure_of_merit] = Models.specificity_score(df_predict.loc[val_idx,'target'], \n",
    "                                                                         df_predict.loc[val_idx,'model_output'])\n",
    "        if figure_of_merit == 'acc':\n",
    "            df_train.loc[ifold,figure_of_merit] = Models.acc_score(df_predict.loc[trn_idx,'target'], \n",
    "                                                                   df_predict.loc[trn_idx,'model_output'])\n",
    "            df_val.loc[ifold,figure_of_merit] = Models.acc_score(df_predict.loc[val_idx,'target'], \n",
    "                                                                         df_predict.loc[val_idx,'model_output'])\n",
    "        if figure_of_merit == 'sp':\n",
    "            df_train.loc[ifold,figure_of_merit] = Models.sp_index(df_predict.loc[trn_idx,'target'], \n",
    "                                                                  df_predict.loc[trn_idx,'model_output'])\n",
    "            df_val.loc[ifold,figure_of_merit] = Models.sp_index(df_predict.loc[val_idx,'target'], \n",
    "                                                                df_predict.loc[val_idx,'model_output'])\n",
    "df_fig_of_merit = pd.concat([df_train.mean(axis=0).T,df_train.std(axis=0).T],axis=1)\n",
    "df_fig_of_merit = df_fig_of_merit.rename(columns={0:'mean',1:'std'}, )\n",
    "df_fig_of_merit = df_fig_of_merit.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90958fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fig_of_merit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c8f69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model_type = 'DecisionTree'\n",
    "\n",
    "dataset_name = 'pos_test'#'personal', 'social', 'clinical', 'pos_test', 'PeSo', 'PeSoCli', 'PeCli', 'CliPos', 'PeSoCliPos_par'\n",
    "figure_of_merit_names = ['sens', 'spec', 'acc', 'sp']\n",
    "\n",
    "n_folds =  df_config['cv_folds'][train_id]\n",
    "cv_path = df_config['cv_path'][train_id]\n",
    "pipe_path = df_config['pipeline_path'][train_id]\n",
    "model_path = df_config['model_path'][train_id]\n",
    "\n",
    "for ifold in range(df_config['cv_folds'][train_id]):\n",
    "    print('Processing fold %i of %i'%(ifold, df_config['cv_folds'][train_id]))\n",
    "    \n",
    "    cv_name = '%s_%s_CV_fold_%i_of_%i_cv_indexes.pkl'%(df_config['hash_id'][train_id],\n",
    "                                                         df_config['cv_alg'][train_id],\n",
    "                                                         ifold, n_folds)\n",
    "    with open(os.path.join(cv_path,cv_name),'rb') as file_handler:\n",
    "        [trn_idx,val_idx] = pickle.load(file_handler)\n",
    "    \n",
    "    model_name = '%s_%s_%i_fold_model_%s.pkl'%(df_config['hash_id'][train_id],\n",
    "                                               model_type, ifold, \n",
    "                                               dataset_name)\n",
    "    model = pickle.load(open(os.path.join(model_path, model_name), 'rb'))\n",
    "    fn=list(dev_pos_test.columns)\n",
    "    cn=['TB+', 'TB-']\n",
    "    tree.plot_tree(model,feature_names = fn,class_names=cn,filled = True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7100e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c4aa1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894554e3",
   "metadata": {},
   "source": [
    "# Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2ed513",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "model_type = 'MLPNeuralNetwork'\n",
    "n_folds =  df_config['cv_folds'][train_id]\n",
    "\n",
    "cv_path = df_config['cv_path'][train_id]\n",
    "pipe_path = df_config['pipeline_path'][train_id]\n",
    "\n",
    "n_folds = df_config['cv_folds'][train_id]\n",
    "hidden_neurons = list(get_list_of_hidden_neurons(df_config['model_neurons'][train_id]))\n",
    "\n",
    "datasets_data = [dev_social, dev_clinical, dev_pos_test, dev_personal]\n",
    "datasets_name = ['social', 'clinical', 'pos_test', 'personal']\n",
    "\n",
    "# para desenvolvimento\n",
    "#datasets_data = [dev_social]\n",
    "#datasets_name = ['social']\n",
    "\n",
    "\n",
    "print(get_train_description(df_config, train_id))\n",
    "\n",
    "for idataset, dataset_name in enumerate(datasets_name): \n",
    "    print('Processing %s'%(dataset_name))\n",
    "    data = datasets_data[idataset]\n",
    "    for ifold in range(n_folds):\n",
    "        #if ifold != 0: # para desenvolvimento\n",
    "        #    break\n",
    "        \n",
    "        print('Training %i fold of %i folds\\n\\n'%(ifold+1, n_folds))\n",
    "\n",
    "        cv_name = '%s_%s_CV_fold_%i_of_%i_cv_indexes.pkl'%(df_config['hash_id'][train_id],\n",
    "                                                         df_config['cv_alg'][train_id],\n",
    "                                                         ifold, n_folds)\n",
    "        with open(os.path.join(cv_path,cv_name),'rb') as file_handler:\n",
    "            [trn_idx,val_idx] = pickle.load(file_handler)\n",
    "\n",
    "        pipe_name = '%s_%s_CV_fold_%i_of_%i_pipe_%s.jbl'%(df_config['hash_id'][train_id],\n",
    "                                                          df_config['cv_alg'][train_id],\n",
    "                                                          ifold, n_folds, \n",
    "                                                          datasets_name[idataset])\n",
    "        with open(os.path.join(pipe_path,pipe_name),'rb') as file_handler:\n",
    "            pipe = joblib.load(file_handler)\n",
    "\n",
    "        trn_data = pipe.transform(data)\n",
    "        trn_trgt = dev_target.values # tf.keras.utils.to_categorical(dev_target, num_classes=len(np.unique(dev_target)))\n",
    "        for idx, ineuron in enumerate(hidden_neurons):\n",
    "            print('Training for %i neuron in'%(ineuron),hidden_neurons)\n",
    "            for iinit in range(df_config['model_inits'][train_id]):\n",
    "                print('Training for %i init in %i inits'%(iinit+1, df_config['model_inits'][train_id]))\n",
    "                model_name = '%s_%s_%i_fold_%i_neuron_%i_init_model_%s.pkl'%(df_config['hash_id'][train_id],\n",
    "                                                                             model_type,ifold, \n",
    "                                                                             ineuron, iinit,\n",
    "                                                                             datasets_name[idataset])\n",
    "                model_path = df_config['model_path'][train_id]\n",
    "                if os.path.exists(os.path.join(model_path, model_name)):\n",
    "                    print('Modelo existente em %s'%(os.path.join(model_path, model_name)))\n",
    "                    #model = Models.MLPModel(n_hidden_neurons=ineuron,verbose=2)\n",
    "                    #model.load(os.path.join(model_path, model_name))\n",
    "                else:\n",
    "                    print('Modelo não existe\\n\\n')\n",
    "                    model = Models.MLPModel(n_hidden_neurons=ineuron,verbose=2)\n",
    "                    model.fit(trn_data, trn_trgt, trn_id=trn_idx, val_id=val_idx, \n",
    "                              epochs=100,#df_config['model_epochs'][train_id], \n",
    "                              random_state=iinit, \n",
    "                              learning_rate=0.01,#df_config['model_learning_rate'][train_id],\n",
    "                              patience=100,#df_config['model_patience'][train_id],\n",
    "                              batch_size=int(df_config['model_batch_size'][train_id]),\n",
    "                             )\n",
    "                    predictions = model.predict(trn_data)\n",
    "                    df_predict = pd.DataFrame(data=np.concatenate((dev_target.values, \n",
    "                                                                   predictions,\n",
    "                                                                   np.sign(predictions)),\n",
    "                                                                  axis=1), \n",
    "                                              columns=['target', 'nn_output', 'prediction'])\n",
    "                    prediction_name = '%s_%s_%i_fold_%i_neuron_%i_init_prediction_file_%s.csv'%(df_config['hash_id'][train_id],\n",
    "                                                                                                model_type,ifold, ineuron, iinit,\n",
    "                                                                                                datasets_name[idataset])\n",
    "                    df_predict.to_csv(os.path.join(model_path, prediction_name),index=False)\n",
    "                    model.save(os.path.join(model_path, model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759346f0",
   "metadata": {},
   "source": [
    "# Análise de Topologia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d8caf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from AuxiliarFunctions import *\n",
    "\n",
    "\n",
    "print(get_train_description(df_config, train_id))\n",
    "\n",
    "\n",
    "if True: # remova quando tiver segurança no treinamento\n",
    "    model_type = 'MLPNeuralNetwork'\n",
    "    \n",
    "    datasets_data = [dev_social, dev_clinical, dev_pos_test, dev_personal]\n",
    "    datasets_name = ['social', 'clinical', 'pos_test', 'personal']\n",
    "\n",
    "    \n",
    "    hidden_neurons = list(get_list_of_hidden_neurons(df_config['model_neurons'][train_id]))\n",
    "    \n",
    "    n_folds = df_config['cv_folds'][train_id]\n",
    "    \n",
    "    figure_of_merit_matrix = np.zeros([len(datasets_name),len(hidden_neurons),n_folds])\n",
    "    best_init_matrix = np.zeros([len(datasets_name),len(hidden_neurons),n_folds])\n",
    "    \n",
    "    for idataset, dataset_name in enumerate(datasets_name): \n",
    "        print('Processing %s'%(dataset_name))\n",
    "        data = datasets_data[idataset]\n",
    "    \n",
    "        for ifold in range(n_folds):\n",
    "            print('Analysing %i fold of %i folds'%(ifold+1, n_folds))\n",
    "\n",
    "            for idx, ineuron in enumerate(hidden_neurons):\n",
    "                print('Analysing for %i neuron in'%(ineuron),hidden_neurons)\n",
    "\n",
    "                best_figure_of_merit = -9999\n",
    "\n",
    "                for iinit in range(df_config['model_inits'][train_id]):\n",
    "                    print('Analysing for %i init in %i inits'%(iinit+1, df_config['model_inits'][train_id]))\n",
    "                    # repare que mudou!\n",
    "                    model_name = '%s_%s_%i_fold_%i_neuron_%i_init_prediction_file_%s.csv'%(df_config['hash_id'][train_id],\n",
    "                                                                                           model_type,ifold, ineuron, iinit,\n",
    "                                                                                           datasets_name[idataset])\n",
    "                    model_path = df_config['model_path'][train_id]\n",
    "                    #print(os.path.join(model_path, model_name))\n",
    "                    if os.path.exists(os.path.join(model_path, model_name)):\n",
    "                        print('Modelo existente em %s'%(os.path.join(model_path, model_name)))\n",
    "                        # adicionar a função de analise acc ou outra\n",
    "                        df_predictions = pd.read_csv(os.path.join(model_path, model_name))\n",
    "                        figure_of_merit = Models.acc_score(df_predictions['target'].values, \n",
    "                                                           df_predictions['prediction'].values)\n",
    "                        print('Figure of Merit: Acc, value %1.3f'%(figure_of_merit))\n",
    "                        if best_figure_of_merit < figure_of_merit:\n",
    "                            figure_of_merit_matrix[idataset,idx, ifold] = figure_of_merit\n",
    "                            best_figure_of_merit = figure_of_merit\n",
    "                            best_init_matrix[idataset,idx,ifold] = iinit\n",
    "\n",
    "                    else:\n",
    "                        print('Modelo não existe\\n\\n')\n",
    "                        # fora!\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba18bf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "figure_of_merit_label = 'Accuracy'\n",
    "\n",
    "for idataset, dataset_name in enumerate(datasets_name): \n",
    "    plt.figure(figsize=(6, 6))\n",
    "    hidden_neurons = list(get_list_of_hidden_neurons(df_config['model_neurons'][train_id]))\n",
    "    mean_figure_of_merit = np.mean(figure_of_merit_matrix[idataset,:,:],axis=1)\n",
    "    std_figure_of_merit = np.std(figure_of_merit_matrix[idataset,:,:],axis=1) \n",
    "\n",
    "    linestyle = {\"linestyle\":\"-\", \"linewidth\":1, \"markersize\":2.5, \"markeredgewidth\":1, \"elinewidth\":1, \"capsize\":5}\n",
    "\n",
    "    plt.errorbar(hidden_neurons, mean_figure_of_merit, yerr=std_figure_of_merit,\n",
    "                 color=\"k\", fmt='o',**linestyle, label='Figure of Merit')\n",
    "    plt.title('%s - %s vs Neurons analysis'%(dataset_name,figure_of_merit_label))\n",
    "    plt.ylabel('%s (percentage)'%(figure_of_merit_label))\n",
    "    plt.xlabel('Number of Neurons')\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e369621",
   "metadata": {},
   "source": [
    "# Treinamento da Fusão - PeSo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475413ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'MLPNeuralNetwork'\n",
    "n_folds =  df_config['cv_folds'][train_id]\n",
    "\n",
    "cv_path = df_config['cv_path'][train_id]\n",
    "pipe_path = df_config['pipeline_path'][train_id]\n",
    "\n",
    "hidden_neurons = list(get_list_of_hidden_neurons(df_config['model_neurons'][train_id]))\n",
    "\n",
    "datasets_data = [dev_personal, dev_social]\n",
    "datasets_name = ['personal', 'social']\n",
    "\n",
    "# numero de neuronios de cada modelo que vai realizar a fusão\n",
    "previous_neuron = [2,2]\n",
    "previous_init = [0,0]\n",
    "df_fusion = pd.DataFrame()\n",
    "\n",
    "print(get_train_description(df_config, train_id))\n",
    "\n",
    "for ifold in range(n_folds):\n",
    "    #if ifold != 0: # para desenvolvimento\n",
    "    #    break\n",
    "    print('Training %i fold of %i folds\\n\\n'%(ifold+1, n_folds))\n",
    "    cv_name = '%s_%s_CV_fold_%i_of_%i_cv_indexes.pkl'%(df_config['hash_id'][train_id],\n",
    "                                                         df_config['cv_alg'][train_id],\n",
    "                                                         ifold, n_folds)\n",
    "    with open(os.path.join(cv_path,cv_name),'rb') as file_handler:\n",
    "        [trn_idx,val_idx] = pickle.load(file_handler)\n",
    "        \n",
    "    # agora eu tenho que fazer a saída de cada um dos modelos base e gerar um banco novo\n",
    "    for idataset, dataset_name in enumerate(datasets_name):  \n",
    "        print('Processing %s'%(dataset_name))\n",
    "        data = datasets_data[idataset]\n",
    "        # carregar o pipeline de processamento\n",
    "        pipe_name = '%s_%s_CV_fold_%i_of_%i_pipe_%s.jbl'%(df_config['hash_id'][train_id],\n",
    "                                                          df_config['cv_alg'][train_id],\n",
    "                                                          ifold, n_folds, \n",
    "                                                          datasets_name[idataset])\n",
    "        with open(os.path.join(pipe_path,pipe_name),'rb') as file_handler:\n",
    "            pipe = joblib.load(file_handler)\n",
    "        trn_data = pipe.transform(data)\n",
    "        \n",
    "        #carregar o modelo certo\n",
    "        model_name = '%s_%s_%i_fold_%i_neuron_%i_init_model_%s.pkl'%(df_config['hash_id'][train_id],\n",
    "                                                                     model_type,ifold, \n",
    "                                                                     previous_neuron[idataset], \n",
    "                                                                     previous_init[idataset],\n",
    "                                                                     datasets_name[idataset])\n",
    "        model_path = df_config['model_path'][train_id]\n",
    "        if os.path.exists(os.path.join(model_path, model_name)):\n",
    "            print('Modelo existente em %s'%(os.path.join(model_path, model_name)))\n",
    "            model = Models.MLPModel(n_hidden_neurons=previous_neuron[idataset],verbose=2)\n",
    "            model.load(os.path.join(model_path, model_name))\n",
    "            processed_data = model.predict_one_layer_before_output(trn_data)\n",
    "            df_buffer = pd.DataFrame(data=processed_data)\n",
    "            df_buffer = df_buffer.add_prefix('%s_'%(dataset_name))\n",
    "        \n",
    "        if idataset == 0:\n",
    "            df_fusion = df_buffer\n",
    "        else:\n",
    "            df_fusion = pd.concat([df_fusion, df_buffer],axis=1)\n",
    "            \n",
    "    trn_data = df_fusion.values\n",
    "    trn_trgt = dev_target.values\n",
    "    dataset_name = 'PeSo'\n",
    "    \n",
    "    for idx, ineuron in enumerate(hidden_neurons):\n",
    "        print('Training for %i neuron in'%(ineuron),hidden_neurons)\n",
    "        for iinit in range(df_config['model_inits'][train_id]):\n",
    "            print('Training for %i init in %i inits'%(iinit+1, df_config['model_inits'][train_id]))\n",
    "            model_name = '%s_%s_%i_fold_%i_neuron_%i_init_model_%s.pkl'%(df_config['hash_id'][train_id],\n",
    "                                                                         model_type,ifold, \n",
    "                                                                         ineuron, iinit,\n",
    "                                                                         dataset_name)\n",
    "            if os.path.exists(os.path.join(model_path, model_name)):\n",
    "                print('Modelo existente em %s'%(os.path.join(model_path, model_name)))\n",
    "            else:\n",
    "                print('Modelo não existe\\n\\n')\n",
    "                model = Models.MLPModel(n_hidden_neurons=ineuron,verbose=2)\n",
    "                model.fit(trn_data, trn_trgt, trn_id=trn_idx, val_id=val_idx, \n",
    "                          epochs=1000,#df_config['model_epochs'][train_id], \n",
    "                          random_state=iinit, \n",
    "                          learning_rate=0.01,#df_config['model_learning_rate'][train_id],\n",
    "                          patience=10,#df_config['model_patience'][train_id],\n",
    "                          batch_size=int(df_config['model_batch_size'][train_id]),\n",
    "                         )\n",
    "                predictions = model.predict(trn_data)\n",
    "                df_predict = pd.DataFrame(data=np.concatenate((dev_target.values, \n",
    "                                                               predictions,\n",
    "                                                               np.sign(predictions)),\n",
    "                                                              axis=1), \n",
    "                                          columns=['target', 'nn_output', 'prediction'])\n",
    "                prediction_name = '%s_%s_%i_fold_%i_neuron_%i_init_prediction_file_%s.csv'%(df_config['hash_id'][train_id],\n",
    "                                                                                            model_type,ifold, ineuron, iinit,\n",
    "                                                                                            dataset_name)\n",
    "                df_predict.to_csv(os.path.join(model_path, prediction_name),index=False)\n",
    "                model.save(os.path.join(model_path, model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750011a1",
   "metadata": {},
   "source": [
    "# Treinamento de Fusão - PeSoCli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76d261a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'MLPNeuralNetwork'\n",
    "n_folds =  df_config['cv_folds'][train_id]\n",
    "\n",
    "cv_path = df_config['cv_path'][train_id]\n",
    "pipe_path = df_config['pipeline_path'][train_id]\n",
    "\n",
    "hidden_neurons = list(get_list_of_hidden_neurons(df_config['model_neurons'][train_id]))\n",
    "\n",
    "datasets_data = [dev_personal, dev_social]\n",
    "datasets_name = ['personal', 'social']\n",
    "\n",
    "# numero de neuronios de cada modelo que vai realizar a fusão\n",
    "PeSo_neurons = [2,2]\n",
    "PeSo_inits = [0,0]\n",
    "df_PeSo_fusion = pd.DataFrame()\n",
    "df_PeSo_data = [dev_personal, dev_social]\n",
    "df_PeSo_names = ['personal', 'social']\n",
    "\n",
    "\n",
    "PeSoCli_neurons = [2,3]\n",
    "PeSoCli_inits = [0,0]\n",
    "df_PeSoCli_fusion = pd.DataFrame()\n",
    "\n",
    "model_path = df_config['model_path'][train_id]\n",
    "\n",
    "print(get_train_description(df_config, train_id))\n",
    "\n",
    "for ifold in range(n_folds):\n",
    "    #if ifold != 0: # para desenvolvimento\n",
    "    #    break\n",
    "        \n",
    "    df_buffer = []\n",
    "    print('Training %i fold of %i folds\\n\\n'%(ifold+1, n_folds))\n",
    "    cv_name = '%s_%s_CV_fold_%i_of_%i_cv_indexes.pkl'%(df_config['hash_id'][train_id],\n",
    "                                                         df_config['cv_alg'][train_id],\n",
    "                                                         ifold, n_folds)\n",
    "    with open(os.path.join(cv_path,cv_name),'rb') as file_handler:\n",
    "        [trn_idx,val_idx] = pickle.load(file_handler)\n",
    "        \n",
    "    # agora eu tenho que fazer a saída de cada um dos modelos base e gerar um banco novo\n",
    "    for idataset, dataset_name in enumerate(df_PeSo_names):  \n",
    "        print('Processing %s'%(dataset_name))\n",
    "        data = datasets_data[idataset]\n",
    "        # carregar o pipeline de processamento\n",
    "        pipe_name = '%s_%s_CV_fold_%i_of_%i_pipe_%s.jbl'%(df_config['hash_id'][train_id],\n",
    "                                                          df_config['cv_alg'][train_id],\n",
    "                                                          ifold, n_folds, \n",
    "                                                          datasets_name[idataset])\n",
    "        with open(os.path.join(pipe_path,pipe_name),'rb') as file_handler:\n",
    "            pipe = joblib.load(file_handler)\n",
    "        trn_data = pipe.transform(data)\n",
    "        \n",
    "        #carregar o modelo certo\n",
    "        model_name = '%s_%s_%i_fold_%i_neuron_%i_init_model_%s.pkl'%(df_config['hash_id'][train_id],\n",
    "                                                                     model_type,ifold, \n",
    "                                                                     previous_neuron[idataset], \n",
    "                                                                     previous_init[idataset],\n",
    "                                                                     datasets_name[idataset])\n",
    "        \n",
    "        if os.path.exists(os.path.join(model_path, model_name)):\n",
    "            print('Modelo existente em %s'%(os.path.join(model_path, model_name)))\n",
    "            model = Models.MLPModel(n_hidden_neurons=previous_neuron[idataset],verbose=2)\n",
    "            model.load(os.path.join(model_path, model_name))\n",
    "            processed_data = model.predict_one_layer_before_output(trn_data)\n",
    "            df_buffer = pd.DataFrame(data=processed_data)\n",
    "            df_buffer = df_buffer.add_prefix('%s_'%(dataset_name))\n",
    "        \n",
    "        if idataset == 0:\n",
    "            df_PeSo_fusion = df_buffer\n",
    "        else:\n",
    "            df_PeSo_fusion = pd.concat([df_PeSo_fusion, df_buffer],axis=1)\n",
    " \n",
    "    dataset_name = 'PeSo'   \n",
    "    model_name = '%s_%s_%i_fold_%i_neuron_%i_init_model_%s.pkl'%(df_config['hash_id'][train_id],\n",
    "                                                                 model_type,ifold, \n",
    "                                                                 PeSoCli_neurons[0],\n",
    "                                                                 PeSoCli_inits[0],\n",
    "                                                                 dataset_name)\n",
    "    \n",
    "    df_buffer = []\n",
    "    if os.path.exists(os.path.join(model_path, model_name)):\n",
    "        print('Modelo existente em %s'%(os.path.join(model_path, model_name)))\n",
    "        model = Models.MLPModel(n_hidden_neurons=PeSoCli_neurons[0],verbose=2)\n",
    "        model.load(os.path.join(model_path, model_name))\n",
    "        processed_data = model.predict_one_layer_before_output(df_PeSo_fusion.values)\n",
    "        df_buffer = pd.DataFrame(data=processed_data)\n",
    "        df_buffer = df_buffer.add_prefix('%s_'%(dataset_name))\n",
    "        df_PeSoCli_fusion = df_buffer\n",
    "    df_buffer = []\n",
    "    \n",
    "    dataset_name = 'clinical'   \n",
    "    model_name = '%s_%s_%i_fold_%i_neuron_%i_init_model_%s.pkl'%(df_config['hash_id'][train_id],\n",
    "                                                                 model_type,ifold, \n",
    "                                                                 PeSoCli_neurons[1],\n",
    "                                                                 PeSoCli_inits[1],\n",
    "                                                                 dataset_name)\n",
    "    \n",
    "    \n",
    "    if os.path.exists(os.path.join(model_path, model_name)):\n",
    "        print('Modelo existente em %s'%(os.path.join(model_path, model_name)))\n",
    "        \n",
    "        data = dev_clinical\n",
    "        # carregar o pipeline de processamento\n",
    "        pipe_name = '%s_%s_CV_fold_%i_of_%i_pipe_%s.jbl'%(df_config['hash_id'][train_id],\n",
    "                                                          df_config['cv_alg'][train_id],\n",
    "                                                          ifold, n_folds, \n",
    "                                                          dataset_name)\n",
    "        with open(os.path.join(pipe_path,pipe_name),'rb') as file_handler:\n",
    "            pipe = joblib.load(file_handler)\n",
    "        trn_data = pipe.transform(data)\n",
    "        \n",
    "        model = Models.MLPModel(n_hidden_neurons=PeSoCli_neurons[1],verbose=2)\n",
    "        model.load(os.path.join(model_path, model_name))\n",
    "        processed_data = model.predict_one_layer_before_output(trn_data)\n",
    "        df_buffer = pd.DataFrame(data=processed_data)\n",
    "        df_buffer = df_buffer.add_prefix('%s_'%(dataset_name))\n",
    "        df_PeSoCli_fusion = pd.concat([df_PeSoCli_fusion, df_buffer],axis=1)\n",
    "        \n",
    "        \n",
    "    trn_data = df_PeSoCli_fusion.values\n",
    "    trn_trgt = dev_target.values\n",
    "    dataset_name = 'PeSoCli'\n",
    "    \n",
    "    for idx, ineuron in enumerate(hidden_neurons):\n",
    "        print('Training for %i neuron in'%(ineuron),hidden_neurons)\n",
    "        for iinit in range(df_config['model_inits'][train_id]):\n",
    "            print('Training for %i init in %i inits'%(iinit+1, df_config['model_inits'][train_id]))\n",
    "            model_name = '%s_%s_%i_fold_%i_neuron_%i_init_model_%s.pkl'%(df_config['hash_id'][train_id],\n",
    "                                                                         model_type,ifold, \n",
    "                                                                         ineuron, iinit,\n",
    "                                                                         dataset_name)\n",
    "            if os.path.exists(os.path.join(model_path, model_name)):\n",
    "                print('Modelo existente em %s'%(os.path.join(model_path, model_name)))\n",
    "            else:\n",
    "                print('Modelo não existe\\n\\n')\n",
    "                model = Models.MLPModel(n_hidden_neurons=ineuron,verbose=2)\n",
    "                model.fit(trn_data, trn_trgt, trn_id=trn_idx, val_id=val_idx, \n",
    "                          epochs=1000,#df_config['model_epochs'][train_id], \n",
    "                          random_state=iinit, \n",
    "                          learning_rate=0.01,#df_config['model_learning_rate'][train_id],\n",
    "                          patience=10,#df_config['model_patience'][train_id],\n",
    "                          batch_size=int(df_config['model_batch_size'][train_id]),\n",
    "                         )\n",
    "                predictions = model.predict(trn_data)\n",
    "                df_predict = pd.DataFrame(data=np.concatenate((dev_target.values, \n",
    "                                                               predictions,\n",
    "                                                               np.sign(predictions)),\n",
    "                                                              axis=1), \n",
    "                                          columns=['target', 'nn_output', 'prediction'])\n",
    "                prediction_name = '%s_%s_%i_fold_%i_neuron_%i_init_prediction_file_%s.csv'%(df_config['hash_id'][train_id],\n",
    "                                                                                            model_type,ifold, ineuron, iinit,\n",
    "                                                                                            dataset_name)\n",
    "                df_predict.to_csv(os.path.join(model_path, prediction_name),index=False)\n",
    "                model.save(os.path.join(model_path, model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3286d2a",
   "metadata": {},
   "source": [
    "# Treinamento de Fusão - PeCli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260dcce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'MLPNeuralNetwork'\n",
    "n_folds =  df_config['cv_folds'][train_id]\n",
    "\n",
    "cv_path = df_config['cv_path'][train_id]\n",
    "pipe_path = df_config['pipeline_path'][train_id]\n",
    "\n",
    "hidden_neurons = list(get_list_of_hidden_neurons(df_config['model_neurons'][train_id]))\n",
    "\n",
    "datasets_data = [dev_personal, dev_clinical]\n",
    "datasets_name = ['personal', 'clinical']\n",
    "\n",
    "# numero de neuronios de cada modelo que vai realizar a fusão\n",
    "previous_neuron = [2,3]\n",
    "previous_init = [0,0]\n",
    "df_fusion = pd.DataFrame()\n",
    "\n",
    "print(get_train_description(df_config, train_id))\n",
    "\n",
    "for ifold in range(n_folds):\n",
    "    #if ifold != 0: # para desenvolvimento\n",
    "    #    break\n",
    "    print('Training %i fold of %i folds\\n\\n'%(ifold+1, n_folds))\n",
    "    cv_name = '%s_%s_CV_fold_%i_of_%i_cv_indexes.pkl'%(df_config['hash_id'][train_id],\n",
    "                                                         df_config['cv_alg'][train_id],\n",
    "                                                         ifold, n_folds)\n",
    "    with open(os.path.join(cv_path,cv_name),'rb') as file_handler:\n",
    "        [trn_idx,val_idx] = pickle.load(file_handler)\n",
    "        \n",
    "    # agora eu tenho que fazer a saída de cada um dos modelos base e gerar um banco novo\n",
    "    for idataset, dataset_name in enumerate(datasets_name):  \n",
    "        print('Processing %s'%(dataset_name))\n",
    "        data = datasets_data[idataset]\n",
    "        # carregar o pipeline de processamento\n",
    "        pipe_name = '%s_%s_CV_fold_%i_of_%i_pipe_%s.jbl'%(df_config['hash_id'][train_id],\n",
    "                                                          df_config['cv_alg'][train_id],\n",
    "                                                          ifold, n_folds, \n",
    "                                                          datasets_name[idataset])\n",
    "        with open(os.path.join(pipe_path,pipe_name),'rb') as file_handler:\n",
    "            pipe = joblib.load(file_handler)\n",
    "        trn_data = pipe.transform(data)\n",
    "        \n",
    "        #carregar o modelo certo\n",
    "        model_name = '%s_%s_%i_fold_%i_neuron_%i_init_model_%s.pkl'%(df_config['hash_id'][train_id],\n",
    "                                                                     model_type,ifold, \n",
    "                                                                     previous_neuron[idataset], \n",
    "                                                                     previous_init[idataset],\n",
    "                                                                     datasets_name[idataset])\n",
    "        model_path = df_config['model_path'][train_id]\n",
    "        if os.path.exists(os.path.join(model_path, model_name)):\n",
    "            print('Modelo existente em %s'%(os.path.join(model_path, model_name)))\n",
    "            model = Models.MLPModel(n_hidden_neurons=previous_neuron[idataset],verbose=2)\n",
    "            model.load(os.path.join(model_path, model_name))\n",
    "            processed_data = model.predict_one_layer_before_output(trn_data)\n",
    "            df_buffer = pd.DataFrame(data=processed_data)\n",
    "            df_buffer = df_buffer.add_prefix('%s_'%(dataset_name))\n",
    "        \n",
    "        if idataset == 0:\n",
    "            df_fusion = df_buffer\n",
    "        else:\n",
    "            df_fusion = pd.concat([df_fusion, df_buffer],axis=1)\n",
    "            \n",
    "    trn_data = df_fusion.values\n",
    "    trn_trgt = dev_target.values\n",
    "    dataset_name = 'PeCli'\n",
    "    \n",
    "    for idx, ineuron in enumerate(hidden_neurons):\n",
    "        print('Training for %i neuron in'%(ineuron),hidden_neurons)\n",
    "        for iinit in range(df_config['model_inits'][train_id]):\n",
    "            print('Training for %i init in %i inits'%(iinit+1, df_config['model_inits'][train_id]))\n",
    "            model_name = '%s_%s_%i_fold_%i_neuron_%i_init_model_%s.pkl'%(df_config['hash_id'][train_id],\n",
    "                                                                         model_type,ifold, \n",
    "                                                                         ineuron, iinit,\n",
    "                                                                         dataset_name)\n",
    "            if os.path.exists(os.path.join(model_path, model_name)):\n",
    "                print('Modelo existente em %s'%(os.path.join(model_path, model_name)))\n",
    "            else:\n",
    "                print('Modelo não existe\\n\\n')\n",
    "                model = Models.MLPModel(n_hidden_neurons=ineuron,verbose=2)\n",
    "                model.fit(trn_data, trn_trgt, trn_id=trn_idx, val_id=val_idx, \n",
    "                          epochs=1000,#df_config['model_epochs'][train_id], \n",
    "                          random_state=iinit, \n",
    "                          learning_rate=0.01,#df_config['model_learning_rate'][train_id],\n",
    "                          patience=10,#df_config['model_patience'][train_id],\n",
    "                          batch_size=int(df_config['model_batch_size'][train_id]),\n",
    "                         )\n",
    "                predictions = model.predict(trn_data)\n",
    "                df_predict = pd.DataFrame(data=np.concatenate((dev_target.values, \n",
    "                                                               predictions,\n",
    "                                                               np.sign(predictions)),\n",
    "                                                              axis=1), \n",
    "                                          columns=['target', 'nn_output', 'prediction'])\n",
    "                prediction_name = '%s_%s_%i_fold_%i_neuron_%i_init_prediction_file_%s.csv'%(df_config['hash_id'][train_id],\n",
    "                                                                                            model_type,ifold, ineuron, iinit,\n",
    "                                                                                            dataset_name)\n",
    "                df_predict.to_csv(os.path.join(model_path, prediction_name),index=False)\n",
    "                model.save(os.path.join(model_path, model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793d8465",
   "metadata": {},
   "source": [
    "# Treinamento de Fusão - CliPos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b797b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'MLPNeuralNetwork'\n",
    "n_folds =  df_config['cv_folds'][train_id]\n",
    "\n",
    "cv_path = df_config['cv_path'][train_id]\n",
    "pipe_path = df_config['pipeline_path'][train_id]\n",
    "\n",
    "hidden_neurons = list(get_list_of_hidden_neurons(df_config['model_neurons'][train_id]))\n",
    "\n",
    "datasets_data = [dev_clinical, dev_pos_test]\n",
    "datasets_name = ['clinical', 'pos_test']\n",
    "\n",
    "# numero de neuronios de cada modelo que vai realizar a fusão\n",
    "previous_neuron = [3,2]\n",
    "previous_init = [0,0]\n",
    "df_fusion = pd.DataFrame()\n",
    "\n",
    "print(get_train_description(df_config, train_id))\n",
    "\n",
    "for ifold in range(n_folds):\n",
    "    #if ifold != 0: # para desenvolvimento\n",
    "    #    break\n",
    "    print('Training %i fold of %i folds\\n\\n'%(ifold+1, n_folds))\n",
    "    cv_name = '%s_%s_CV_fold_%i_of_%i_cv_indexes.pkl'%(df_config['hash_id'][train_id],\n",
    "                                                         df_config['cv_alg'][train_id],\n",
    "                                                         ifold, n_folds)\n",
    "    with open(os.path.join(cv_path,cv_name),'rb') as file_handler:\n",
    "        [trn_idx,val_idx] = pickle.load(file_handler)\n",
    "        \n",
    "    # agora eu tenho que fazer a saída de cada um dos modelos base e gerar um banco novo\n",
    "    for idataset, dataset_name in enumerate(datasets_name):  \n",
    "        print('Processing %s'%(dataset_name))\n",
    "        data = datasets_data[idataset]\n",
    "        # carregar o pipeline de processamento\n",
    "        pipe_name = '%s_%s_CV_fold_%i_of_%i_pipe_%s.jbl'%(df_config['hash_id'][train_id],\n",
    "                                                          df_config['cv_alg'][train_id],\n",
    "                                                          ifold, n_folds, \n",
    "                                                          datasets_name[idataset])\n",
    "        with open(os.path.join(pipe_path,pipe_name),'rb') as file_handler:\n",
    "            pipe = joblib.load(file_handler)\n",
    "        trn_data = pipe.transform(data)\n",
    "        \n",
    "        #carregar o modelo certo\n",
    "        model_name = '%s_%s_%i_fold_%i_neuron_%i_init_model_%s.pkl'%(df_config['hash_id'][train_id],\n",
    "                                                                     model_type,ifold, \n",
    "                                                                     previous_neuron[idataset], \n",
    "                                                                     previous_init[idataset],\n",
    "                                                                     datasets_name[idataset])\n",
    "        model_path = df_config['model_path'][train_id]\n",
    "        if os.path.exists(os.path.join(model_path, model_name)):\n",
    "            print('Modelo existente em %s'%(os.path.join(model_path, model_name)))\n",
    "            model = Models.MLPModel(n_hidden_neurons=previous_neuron[idataset],verbose=2)\n",
    "            model.load(os.path.join(model_path, model_name))\n",
    "            processed_data = model.predict_one_layer_before_output(trn_data)\n",
    "            df_buffer = pd.DataFrame(data=processed_data)\n",
    "            df_buffer = df_buffer.add_prefix('%s_'%(dataset_name))\n",
    "        \n",
    "        if idataset == 0:\n",
    "            df_fusion = df_buffer\n",
    "        else:\n",
    "            df_fusion = pd.concat([df_fusion, df_buffer],axis=1)\n",
    "            \n",
    "    trn_data = df_fusion.values\n",
    "    trn_trgt = dev_target.values\n",
    "    dataset_name = 'CliPos'\n",
    "    \n",
    "    for idx, ineuron in enumerate(hidden_neurons):\n",
    "        print('Training for %i neuron in'%(ineuron),hidden_neurons)\n",
    "        for iinit in range(df_config['model_inits'][train_id]):\n",
    "            print('Training for %i init in %i inits'%(iinit+1, df_config['model_inits'][train_id]))\n",
    "            model_name = '%s_%s_%i_fold_%i_neuron_%i_init_model_%s.pkl'%(df_config['hash_id'][train_id],\n",
    "                                                                         model_type,ifold, \n",
    "                                                                         ineuron, iinit,\n",
    "                                                                         dataset_name)\n",
    "            if os.path.exists(os.path.join(model_path, model_name)):\n",
    "                print('Modelo existente em %s'%(os.path.join(model_path, model_name)))\n",
    "            else:\n",
    "                print('Modelo não existe\\n\\n')\n",
    "                model = Models.MLPModel(n_hidden_neurons=ineuron,verbose=2)\n",
    "                model.fit(trn_data, trn_trgt, trn_id=trn_idx, val_id=val_idx, \n",
    "                          epochs=1000,#df_config['model_epochs'][train_id], \n",
    "                          random_state=iinit, \n",
    "                          learning_rate=0.01,#df_config['model_learning_rate'][train_id],\n",
    "                          patience=10,#df_config['model_patience'][train_id],\n",
    "                          batch_size=int(df_config['model_batch_size'][train_id]),\n",
    "                         )\n",
    "                predictions = model.predict(trn_data)\n",
    "                df_predict = pd.DataFrame(data=np.concatenate((dev_target.values, \n",
    "                                                               predictions,\n",
    "                                                               np.sign(predictions)),\n",
    "                                                              axis=1), \n",
    "                                          columns=['target', 'nn_output', 'prediction'])\n",
    "                prediction_name = '%s_%s_%i_fold_%i_neuron_%i_init_prediction_file_%s.csv'%(df_config['hash_id'][train_id],\n",
    "                                                                                            model_type,ifold, ineuron, iinit,\n",
    "                                                                                            dataset_name)\n",
    "                df_predict.to_csv(os.path.join(model_path, prediction_name),index=False)\n",
    "                model.save(os.path.join(model_path, model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292a6c58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a621adb5",
   "metadata": {},
   "source": [
    "# Treinamento de Fusão - PeSoCliPos (Cascateado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade81bdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3944aad",
   "metadata": {},
   "source": [
    "# Treinamento de Fusão - PeSoCliPos (Paralelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8262d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'MLPNeuralNetwork'\n",
    "n_folds =  df_config['cv_folds'][train_id]\n",
    "\n",
    "cv_path = df_config['cv_path'][train_id]\n",
    "pipe_path = df_config['pipeline_path'][train_id]\n",
    "\n",
    "hidden_neurons = list(get_list_of_hidden_neurons(df_config['model_neurons'][train_id]))\n",
    "\n",
    "datasets_data = [dev_personal, dev_social, dev_clinical, dev_pos_test]\n",
    "datasets_name = ['personal', 'social', 'clinical', 'pos_test']\n",
    "\n",
    "# numero de neuronios de cada modelo que vai realizar a fusão\n",
    "previous_neuron = [2, 2, 3, 2]\n",
    "previous_init = [0, 0, 0, 0]\n",
    "df_fusion = pd.DataFrame()\n",
    "\n",
    "print(get_train_description(df_config, train_id))\n",
    "\n",
    "for ifold in range(n_folds):\n",
    "    #if ifold != 0: # para desenvolvimento\n",
    "    #    break\n",
    "    print('Training %i fold of %i folds\\n\\n'%(ifold+1, n_folds))\n",
    "    cv_name = '%s_%s_CV_fold_%i_of_%i_cv_indexes.pkl'%(df_config['hash_id'][train_id],\n",
    "                                                         df_config['cv_alg'][train_id],\n",
    "                                                         ifold, n_folds)\n",
    "    with open(os.path.join(cv_path,cv_name),'rb') as file_handler:\n",
    "        [trn_idx,val_idx] = pickle.load(file_handler)\n",
    "        \n",
    "    # agora eu tenho que fazer a saída de cada um dos modelos base e gerar um banco novo\n",
    "    for idataset, dataset_name in enumerate(datasets_name):  \n",
    "        print('Processing %s'%(dataset_name))\n",
    "        data = datasets_data[idataset]\n",
    "        # carregar o pipeline de processamento\n",
    "        pipe_name = '%s_%s_CV_fold_%i_of_%i_pipe_%s.jbl'%(df_config['hash_id'][train_id],\n",
    "                                                          df_config['cv_alg'][train_id],\n",
    "                                                          ifold, n_folds, \n",
    "                                                          datasets_name[idataset])\n",
    "        with open(os.path.join(pipe_path,pipe_name),'rb') as file_handler:\n",
    "            pipe = joblib.load(file_handler)\n",
    "        trn_data = pipe.transform(data)\n",
    "        \n",
    "        #carregar o modelo certo\n",
    "        model_name = '%s_%s_%i_fold_%i_neuron_%i_init_model_%s.pkl'%(df_config['hash_id'][train_id],\n",
    "                                                                     model_type,ifold, \n",
    "                                                                     previous_neuron[idataset], \n",
    "                                                                     previous_init[idataset],\n",
    "                                                                     datasets_name[idataset])\n",
    "        model_path = df_config['model_path'][train_id]\n",
    "        if os.path.exists(os.path.join(model_path, model_name)):\n",
    "            print('Modelo existente em %s'%(os.path.join(model_path, model_name)))\n",
    "            model = Models.MLPModel(n_hidden_neurons=previous_neuron[idataset],verbose=2)\n",
    "            model.load(os.path.join(model_path, model_name))\n",
    "            processed_data = model.predict_one_layer_before_output(trn_data)\n",
    "            df_buffer = pd.DataFrame(data=processed_data)\n",
    "            df_buffer = df_buffer.add_prefix('%s_'%(dataset_name))\n",
    "        \n",
    "        if idataset == 0:\n",
    "            df_fusion = df_buffer\n",
    "        else:\n",
    "            df_fusion = pd.concat([df_fusion, df_buffer],axis=1)\n",
    "            \n",
    "    trn_data = df_fusion.values\n",
    "    trn_trgt = dev_target.values\n",
    "    dataset_name = 'PeSoCliPos_par'\n",
    "    \n",
    "    for idx, ineuron in enumerate(hidden_neurons):\n",
    "        print('Training for %i neuron in'%(ineuron),hidden_neurons)\n",
    "        for iinit in range(df_config['model_inits'][train_id]):\n",
    "            print('Training for %i init in %i inits'%(iinit+1, df_config['model_inits'][train_id]))\n",
    "            model_name = '%s_%s_%i_fold_%i_neuron_%i_init_model_%s.pkl'%(df_config['hash_id'][train_id],\n",
    "                                                                         model_type,ifold, \n",
    "                                                                         ineuron, iinit,\n",
    "                                                                         dataset_name)\n",
    "            if os.path.exists(os.path.join(model_path, model_name)):\n",
    "                print('Modelo existente em %s'%(os.path.join(model_path, model_name)))\n",
    "            else:\n",
    "                print('Modelo não existe\\n\\n')\n",
    "                model = Models.MLPModel(n_hidden_neurons=ineuron,verbose=2)\n",
    "                model.fit(trn_data, trn_trgt, trn_id=trn_idx, val_id=val_idx, \n",
    "                          epochs=1000,#df_config['model_epochs'][train_id], \n",
    "                          random_state=iinit, \n",
    "                          learning_rate=0.01,#df_config['model_learning_rate'][train_id],\n",
    "                          patience=10,#df_config['model_patience'][train_id],\n",
    "                          batch_size=int(df_config['model_batch_size'][train_id]),\n",
    "                         )\n",
    "                predictions = model.predict(trn_data)\n",
    "                df_predict = pd.DataFrame(data=np.concatenate((dev_target.values, \n",
    "                                                               predictions,\n",
    "                                                               np.sign(predictions)),\n",
    "                                                              axis=1), \n",
    "                                          columns=['target', 'nn_output', 'prediction'])\n",
    "                prediction_name = '%s_%s_%i_fold_%i_neuron_%i_init_prediction_file_%s.csv'%(df_config['hash_id'][train_id],\n",
    "                                                                                            model_type,ifold, ineuron, iinit,\n",
    "                                                                                            dataset_name)\n",
    "                df_predict.to_csv(os.path.join(model_path, prediction_name),index=False)\n",
    "                model.save(os.path.join(model_path, model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c63ce0",
   "metadata": {},
   "source": [
    "# Análise Básica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58edb54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'MLPNeuralNetwork'\n",
    "\n",
    "ineuron = 5\n",
    "iinit = 0\n",
    "dataset_name = 'PeCli'#'personal', 'social', 'clinical', 'pos_test', 'PeSo', 'PeSoCli', 'PeCli', 'CliPos', 'PeSoCliPos_par'\n",
    "figure_of_merit_names = ['sens', 'spec', 'acc', 'sp']\n",
    "\n",
    "n_folds =  df_config['cv_folds'][train_id]\n",
    "cv_path = df_config['cv_path'][train_id]\n",
    "pipe_path = df_config['pipeline_path'][train_id]\n",
    "model_path = df_config['model_path'][train_id]\n",
    "\n",
    "df_train = pd.DataFrame(data=np.zeros([df_config['cv_folds'][train_id],len(figure_of_merit_names)]), columns=figure_of_merit_names)\n",
    "df_val = pd.DataFrame(np.zeros([df_config['cv_folds'][train_id],len(figure_of_merit_names)]))\n",
    "\n",
    "for ifold in range(df_config['cv_folds'][train_id]):\n",
    "    print('Processing fold %i of %i'%(ifold, df_config['cv_folds'][train_id]))\n",
    "    \n",
    "    cv_name = '%s_%s_CV_fold_%i_of_%i_cv_indexes.pkl'%(df_config['hash_id'][train_id],\n",
    "                                                         df_config['cv_alg'][train_id],\n",
    "                                                         ifold, n_folds)\n",
    "    with open(os.path.join(cv_path,cv_name),'rb') as file_handler:\n",
    "        [trn_idx,val_idx] = pickle.load(file_handler)\n",
    "    \n",
    "    prediction_file_name = '%s_%s_%i_fold_%i_neuron_%i_init_prediction_file_%s.csv'%(df_config['hash_id'][train_id],\n",
    "                                                                                     model_type,ifold, ineuron, iinit,\n",
    "                                                                                     dataset_name)\n",
    "    print('Reading: ',prediction_file_name)\n",
    "    df_predict = pd.read_csv(os.path.join(model_path, prediction_file_name))\n",
    "    #print(df_predict['nn_output'].head())\n",
    "    #print('\\n')\n",
    "    for figure_of_merit in figure_of_merit_names:\n",
    "        \n",
    "        if figure_of_merit == 'sens':\n",
    "            df_train.loc[ifold,figure_of_merit] = Models.sensitivity_score(df_predict.loc[trn_idx,'target'], \n",
    "                                                                           df_predict.loc[trn_idx,'prediction'])\n",
    "            df_val.loc[ifold,figure_of_merit] = Models.sensitivity_score(df_predict.loc[val_idx,'target'], \n",
    "                                                                         df_predict.loc[val_idx,'prediction'])\n",
    "        if figure_of_merit == 'spec':\n",
    "            df_train.loc[ifold,figure_of_merit] = Models.specificity_score(df_predict.loc[trn_idx,'target'], \n",
    "                                                                           df_predict.loc[trn_idx,'prediction'])\n",
    "            df_val.loc[ifold,figure_of_merit] = Models.specificity_score(df_predict.loc[val_idx,'target'], \n",
    "                                                                         df_predict.loc[val_idx,'prediction'])\n",
    "        if figure_of_merit == 'acc':\n",
    "            df_train.loc[ifold,figure_of_merit] = Models.acc_score(df_predict.loc[trn_idx,'target'], \n",
    "                                                                   df_predict.loc[trn_idx,'prediction'])\n",
    "            df_val.loc[ifold,figure_of_merit] = Models.acc_score(df_predict.loc[val_idx,'target'], \n",
    "                                                                         df_predict.loc[val_idx,'prediction'])\n",
    "        if figure_of_merit == 'sp':\n",
    "            df_train.loc[ifold,figure_of_merit] = Models.sp_index(df_predict.loc[trn_idx,'target'], \n",
    "                                                                  df_predict.loc[trn_idx,'prediction'])\n",
    "            df_val.loc[ifold,figure_of_merit] = Models.sp_index(df_predict.loc[val_idx,'target'], \n",
    "                                                                df_predict.loc[val_idx,'prediction'])\n",
    "df_fig_of_merit = pd.concat([df_train.mean(axis=0).T,df_train.std(axis=0).T],axis=1)\n",
    "df_fig_of_merit = df_fig_of_merit.rename(columns={0:'mean',1:'std'}, )\n",
    "df_fig_of_merit = df_fig_of_merit.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48410f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fig_of_merit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
